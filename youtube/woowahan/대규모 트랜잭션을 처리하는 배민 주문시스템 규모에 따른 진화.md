https://www.youtube.com/watch?v=704qQs6KoUk&list=WL&index=21&t=615s

배민은 점심, 저녁 시간대에 주문이 몰림.

MSA, 버스트 트래픽
-> 이벤트 기반 통신

**지속되는 성장 속 성장통**
- 단일 장애 포인트
- 대용량 데이터
- 대규모 트랜잭션
- 복잡한 이벤트 아키텍처

### 단일 장애 포인트
'루비'라 불리는 중앙 집중 저장소에 모든 시스템이 의존

특정 시스템 장애 -> 루비 부하 -> 루비에 의존하는 서비스들 장애

**탈루비 프로젝트**  
중앙 시스템에서 분리하고, 각 서비스는 MQ를 기반으로 통신

### 대용량 데이터
주문 내용 저장과 조회가 함께 일어남.

주문내역 정보를 보여주기 위해 많은 정보가 필요함.

주문 애거리거트(루트 엔티티) 기반 join 연산

-> 역정규화 + MongoDB 사용  
주문 데이터 조회는 주문 id만으로 조회 가능해짐.

### 대규모 트랜잭션
주문 DB의 분당 쓰기 처리량 한계 도달.  
쓰기 요청 DB 1개, 조회 DB n개(Replica)  
쓰기 요청 증가는 스펙업으로밖에 대응 불가

- 주문 DB 샤딩
  - 클러스터를 구성하여 쓰기 부하를 지원
- AWS 오로라는 샤딩을 지원하지 않는 엔진
- 애플리케이션 단에서 샤딩을 구현하자!

**샤딩 고민**
- 어느 샤드에 접근할지 결정하는 샤딩 전략 고민
- 여러 샤드에 있는 데이터를 애그리게이트하는 방법

**샤딩 전략**
- Key Based Sharding
  - Shard Key를 이용하여 데이터 소스를 결정하는 방식
- Range Based Sharding
  - 값의 범위 기반으로 데이터를 분산시키는 방식
- Directory Based Sharding
  - 샤드가 어떤 데이터를 가질지 look up table을 유지하는 방식
  
**Key Based Sharding**
주문번호를 해싱해서 샤드를 결정

**장점**
구현이 간단하며 샤드 클러스터 내 샤드들에 골고루 분배 가능

**단점**
장비를 추가, 제거할 때 데이터 재배치가 필요하다.

**Range Based Sharding**
가격 기반 샤딩.

**장점**
구현이 간단

**단점**
데이터가 균등하게 분배되지 않아 특정 샤드에 데이터가 몰려 성능 부하가 발생할 수 있다.

**Directory Based Sharding**
Key Based Sharding과 비슷하지만 중간에 lookup 테이블을 사용

**장점**
- 동적으로 샤드를 추가하는데 유리하다.
- Look up 테이블이 SPOF가 될 수 있따.

**주문 시스템 특징**
- 주문이 정상 동작하지 않으면, 배민 전체의 나쁜 경험
  - 단일 장애 포인트는 피하자
- 동적 주문 데이터는 최대 30일만 저장
  - 샤드 추가 이후 30일이 지나면 데이터는 다시 균등하게 분배된다.

Key Based Sharding 사용을 결정.

주문 순번 % 샤드 수 = 샤드 번호

**구현**
AOP와 AbstractRoutingDataSource를 사용

### 복잡한 이벤트 아키텍처
이벤트 기반 관심사 분리

시스템 관점으로 아키텍처를 볼 때 발생하는 이슈
- 스프링 애플리케이션 이벤트는 로직을 수행하는 주체를 파악하기 어렵
- 이벤트 유실이 발생할 경우 재처리가 어렵

**내부/외부 이벤트 정리**

내부 이벤트는 ZERO Payload 전략. -> 그때그때 필요한 정보를 채워주자.
네트워크 비용보다 이벤트 처리 주체의 단일화에 이점을 둠.

**유실**
이벤트 페이로드를 OutBox에 저장. -> 실패 시 재시도 (중복 발행 O, 유실 X)
